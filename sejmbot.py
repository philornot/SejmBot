#!/usr/bin/env python3
"""
SejmBot - Etap 1: Parser transkrypt√≥w Sejmu RP
Automatycznie pobiera i parsuje transkrypty z posiedze≈Ñ Sejmu.
"""

import hashlib
import json
import logging
import re
import time
from dataclasses import dataclass, asdict
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional, Set
from urllib.parse import urljoin

import requests
# Importy do parsowania r√≥≈ºnych format√≥w
from bs4 import BeautifulSoup

try:
    import pdfplumber

    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

try:
    import docx2txt

    DOCX_SUPPORT = True
except ImportError:
    DOCX_SUPPORT = False

# Opcjonalnie Selenium dla dynamicznych stron
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    SELENIUM_SUPPORT = True
except ImportError:
    SELENIUM_SUPPORT = False


@dataclass
class SejmSession:
    """Reprezentacja posiedzenia Sejmu"""
    session_id: str
    session_number: int
    date: str
    title: str
    url: str
    transcript_url: Optional[str] = None
    transcript_text: Optional[str] = None
    file_type: str = "html"  # html, pdf, docx
    scraped_at: str = ""
    hash: str = ""


class SejmBotConfig:
    """Konfiguracja bota"""

    def __init__(self):
        # todo:
        self.user_agent = "jestem botem :) Mozilla/5.0 (compatible)"
        self.output_dir = Path("transkrypty")
        self.logs_dir = Path("logs")
        self.delay_between_requests = 3  # Zwiƒôkszone dla sejm.gov.pl
        self.max_retries = 3
        self.timeout = 30

        # URLs - zaktualizowane na podstawie aktualnej struktury
        self.base_urls = [
            # X kadencja (aktualna) - mo≈ºe byƒá chroniona
            "https://www.sejm.gov.pl/sejm10.nsf/",
            # IX kadencja - sprawdzone jako dzia≈ÇajƒÖce
            "https://www.sejm.gov.pl/sejm9.nsf/",
            # Alternatywne serwery stenogram√≥w
            "https://orka2.sejm.gov.pl/StenogramyX.nsf/",  # X kadencja
            "https://orka2.sejm.gov.pl/Stenogramy9.nsf/",  # IX kadencja
        ]

        # Wzorce do rozpoznawania link√≥w do transkrypt√≥w - rozszerzone
        self.transcript_patterns = [
            r"stenogram",
            r"sprawozdanie.*stenograficzne",
            r"przebieg.*posiedzenia",
            r"transkrypcja",
            r"protok√≥≈Ç.*obrad",
            r"stenograficzne",
            r"dzie≈Ñ.*posiedzenia"
        ]

        # Dodatkowe nag≈Ç√≥wki dla sejm.gov.pl
        self.additional_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pl,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

        self._setup_directories()
        self._setup_logging()

    def _setup_directories(self):
        """Tworzy potrzebne katalogi"""
        self.output_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)

        # Struktura: transkrypty/YYYY/
        current_year = date.today().year
        (self.output_dir / str(current_year)).mkdir(exist_ok=True)

    def _setup_logging(self):
        """Konfiguruje logowanie"""
        log_file = self.logs_dir / f"sejmbot_{datetime.now().strftime('%Y%m%d')}.log"

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)


class SejmBot:
    """G≈Ç√≥wna klasa bota do pobierania transkrypt√≥w"""

    def __init__(self, config: SejmBotConfig):
        self.config = config
        self.logger = config.logger
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'pl,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })

        # ≈öledzenie ju≈º pobranych sesji
        self.processed_sessions: Set[str] = set()
        self._load_processed_sessions()

        # W≈ÇƒÖcz debug logging je≈õli potrzeba szczeg√≥≈Ç√≥w
        # todo w≈ÇƒÖcz mnie
        # if config.logger.level <= logging.INFO:
        #     config.logger.setLevel(logging.DEBUG)

    def _debug_page_content(self, url: str, soup: BeautifulSoup):
        """Debug: pokazuje co jest na stronie"""
        self.logger.debug(f"=== DEBUG dla {url} ===")

        # Poka≈º wszystkie linki z href
        all_links = soup.find_all('a', href=True)
        self.logger.debug(f"Znaleziono {len(all_links)} link√≥w z href")

        # Poka≈º pierwsze 5 link√≥w jako przyk≈Çad
        for i, link in enumerate(all_links[:10]):
            href = link.get('href', '')
            text = link.get_text(strip=True)[:50]
            classes = link.get('class', [])
            self.logger.debug(f"Link {i + 1}: href='{href[:50]}...' text='{text}' class='{classes}'")

        # Poka≈º linki z class="pdf"
        pdf_links = soup.select('a.pdf')
        self.logger.debug(f"Znaleziono {len(pdf_links)} link√≥w z class='pdf'")

        for i, link in enumerate(pdf_links):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            self.logger.debug(f"PDF {i + 1}: '{text}' -> '{href}'")

        self.logger.debug("=== KONIEC DEBUG ===")

    def _load_processed_sessions(self):
        """≈Åaduje listƒô ju≈º przetworzonych sesji"""
        processed_file = self.config.output_dir / "processed_sessions.json"
        if processed_file.exists():
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_sessions = set(data.get('sessions', []))
                self.logger.info(f"Za≈Çadowano {len(self.processed_sessions)} ju≈º przetworzonych sesji")
            except Exception as e:
                self.logger.warning(f"Nie mo≈ºna za≈Çadowaƒá listy przetworzonych sesji: {e}")

    def _save_processed_session(self, session_id: str):
        """Zapisuje ID sesji jako przetworzonƒÖ"""
        self.processed_sessions.add(session_id)
        processed_file = self.config.output_dir / "processed_sessions.json"

        try:
            data = {
                'sessions': list(self.processed_sessions),
                'last_updated': datetime.now().isoformat()
            }
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"Nie mo≈ºna zapisaƒá listy przetworzonych sesji: {e}")

    def _is_session_link(self, href: str, text: str) -> bool:
        """Sprawdza czy link prowadzi do sesji/stenogramu"""
        href_lower = href.lower()
        text_lower = text.lower()

        # Pozytywne wzorce w URL - dodano PDF i konkretne serwery Sejmu
        positive_url_patterns = [
            'stenogram', 'posiedzenie', 'sesja', 'day_', 'nr_',
            'sprawozdanie', 'transcript', 'meeting',
            'stenointr',  # StenoInter10.nsf
            'ksiazka.pdf',  # pliki stenogram√≥w
            '.pdf',  # wszystkie PDFy
            'steno'  # og√≥lne wzorce steno
        ]

        # Pozytywne wzorce w tek≈õcie - dodano polskie daty
        positive_text_patterns = [
            'posiedzenie', 'stenogram', 'sprawozdanie stenograficzne',
            'sesja', 'dzie≈Ñ', 'nr ', '. posiedzenie', 'stenograficzne',
            'przebieg obrad', 'protok√≥≈Ç',
            # Polskie miesiƒÖce
            'stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',
            'lipca', 'sierpnia', 'wrze≈õnia', 'pa≈∫dziernika', 'listopada', 'grudnia',
            # Dni tygodnia
            'poniedzia≈Çek', 'wtorek', '≈õroda', 'czwartek', 'piƒÖtek', 'sobota', 'niedziela',
            # Wzorce dat
            r'\d{1,2}\s+(stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia)\s+\d{4}',
            # Format numeryczny
            r'\d{4}'  # rok w tek≈õcie
        ]

        # Negatywne wzorce (wykluczenia) - zmniejszone, bo by≈Çy zbyt restrykcyjne
        negative_patterns = [
            'komisja', 'menu', 'nav', 'search', 'wyszukaj', 'login',
            'rss', 'mailto:', 'javascript:', 'tel:',
            'facebook', 'twitter', 'youtube', 'instagram',
            'regulamin', 'kontakt', 'cookie'
        ]

        # Sprawd≈∫ negatywne wzorce
        for pattern in negative_patterns:
            if pattern in href_lower or pattern in text_lower:
                return False

        # Sprawd≈∫ pozytywne wzorce w URL
        url_match = any(pattern in href_lower for pattern in positive_url_patterns)

        # Sprawd≈∫ pozytywne wzorce w tek≈õcie (w tym regex)
        text_match = False
        for pattern in positive_text_patterns:
            if pattern.startswith('r\'') or '\\d' in pattern:  # regex pattern
                import re
                pattern_clean = pattern.replace('r\'', '').replace('\'', '')
                if re.search(pattern_clean, text_lower):
                    text_match = True
                    break
            else:
                if pattern in text_lower:
                    text_match = True
                    break

        # Dodatkowe sprawdzenie: czy tekst wyglƒÖda jak data
        date_like = self._looks_like_date(text)

        return url_match or text_match or date_like

    def _looks_like_date(self, text: str) -> bool:
        """Sprawdza czy tekst wyglƒÖda jak data posiedzenia"""
        import re

        text_lower = text.lower().strip()

        # Wzorce polskich dat
        date_patterns = [
            r'\d{1,2}\s+(stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia)\s+\d{4}',
            r'\d{1,2}\.\d{1,2}\.\d{4}',
            r'\d{4}-\d{1,2}-\d{1,2}',
            # Z dniem tygodnia
            r'\d{1,2}\s+(stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia)\s+\d{4}\s+\((poniedzia≈Çek|wtorek|≈õroda|czwartek|piƒÖtek|sobota|niedziela)\)'
        ]

        for pattern in date_patterns:
            if re.search(pattern, text_lower):
                return True

        # Sprawd≈∫ czy zawiera rok i miesiƒÖc
        has_year = re.search(r'20\d{2}', text_lower)
        polish_months = ['stycznia', 'lutego', 'marca', 'kwietnia', 'maja', 'czerwca',
                         'lipca', 'sierpnia', 'wrze≈õnia', 'pa≈∫dziernika', 'listopada', 'grudnia']
        has_month = any(month in text_lower for month in polish_months)

        return has_year and has_month

    def _try_extract_date_from_url(self, url: str) -> Optional[str]:
        """Pr√≥buje wyciƒÖgnƒÖƒá datƒô z URL"""
        import re

        # Wzorce dat w URL
        date_patterns = [
            r'/(\d{4})/(\d{1,2})/(\d{1,2})',  # /2025/01/15/
            r'date=(\d{4})-(\d{1,2})-(\d{1,2})',  # date=2025-01-15
            r'(\d{4})(\d{2})(\d{2})',  # 20250115
            r'year=(\d{4}).*month=(\d{1,2}).*day=(\d{1,2})'  # parametry URL
        ]

        for pattern in date_patterns:
            match = re.search(pattern, url)
            if match:
                try:
                    if len(match.groups()) == 3:
                        year, month, day = match.groups()
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                except:
                    continue

        return None

    def _make_request(self, url: str, **kwargs) -> Optional[requests.Response]:
        """Wykonuje zapytanie HTTP z retry i error handling"""
        for attempt in range(self.config.max_retries):
            try:
                time.sleep(self.config.delay_between_requests)
                response = self.session.get(
                    url,
                    timeout=self.config.timeout,
                    **kwargs
                )

                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    self.logger.warning(f"Dostƒôp zabroniony (403) dla URL: {url}")
                    return None
                else:
                    self.logger.warning(f"HTTP {response.status_code} dla URL: {url}")

            except requests.RequestException as e:
                self.logger.warning(f"B≈ÇƒÖd zapytania (pr√≥ba {attempt + 1}/{self.config.max_retries}): {e}")
                if attempt < self.config.max_retries - 1:
                    time.sleep(2 ** attempt)  # exponential backoff

        return None

    def _extract_text_from_html(self, html_content: str) -> str:
        """Ekstraktuje tekst z HTML"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # Usuwa zbƒôdne elementy
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()

        # Szuka g≈Ç√≥wnej tre≈õci oraz wzorc√≥w dla stron Sejmu
        content_selectors = [
            # Specjalne dla Sejmu
            'div[id*="stenogram"]',
            'div[class*="stenogram"]',
            'div[class*="transcript"]',
            'div[class*="protokol"]',
            'div[id*="protokol"]',
            '.stenogram-content',
            '.transcript-content',

            # Og√≥lne wzorce
            'main',
            'article',
            '.main-content',
            '.content',
            '#content',
            '.post-content',

            # Fallback - ca≈Ça tre≈õƒá body bez nav/header/footer
            'body'
        ]

        text_content = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # We≈∫ pierwszy znaleziony element
                element = elements[0]
                text_content = element.get_text(separator=' ', strip=True)
                if len(text_content) > 200:  # Sprawd≈∫ czy to sensowna tre≈õƒá
                    self.logger.debug(f"U≈ºyto selektora: {selector}, tekst: {len(text_content)} znak√≥w")
                    break

        # Je≈õli nadal brak tre≈õci, we≈∫ ca≈Çy tekst
        if not text_content or len(text_content) < 200:
            text_content = soup.get_text(separator=' ', strip=True)

        # Czyszczenie tekstu
        import re

        # Usu≈Ñ nadmiarowe bia≈Çe znaki
        text_content = re.sub(r'\s+', ' ', text_content)

        # Usu≈Ñ typowe ≈õmieci z stron gov.pl
        text_content = re.sub(r'(JavaScript.*?w≈ÇƒÖcz|Cookie|Mapa strony|Kontakt)', '', text_content)

        # Usu≈Ñ menu/nawigacjƒô
        text_content = re.sub(r'(Menu g≈Ç√≥wne|Nawigacja|Przejd≈∫ do|Skip to)', '', text_content)

        text_content = text_content.strip()

        self.logger.debug(f"HTML -> tekst: {len(text_content)} znak√≥w")
        return text_content

    def _download_and_parse_file(self, url: str, file_type: str, session_id: str) -> tuple[str, bytes]:
        """
        Pobiera plik i zwraca zar√≥wno tekst jak i oryginalne bajty
        """
        try:
            self.logger.info(f"Pobieranie {file_type.upper()}: {url}")

            response = self._make_request(url)
            if not response:
                return "", b""

            # Pobierz oryginalne bajty
            file_bytes = response.content

            # Sprawd≈∫ content-type
            content_type = response.headers.get('content-type', '').lower()

            if file_type == 'pdf':
                if 'application/pdf' not in content_type and not url.endswith('.pdf'):
                    self.logger.warning(f"URL {url} mo≈ºe nie byƒá PDFem (content-type: {content_type})")

                # Parsuj tekst z PDF
                text = self._extract_text_from_pdf_bytes(file_bytes)
                return text, file_bytes

            elif file_type == 'docx':
                # Dla DOCX nie zapisujemy orygina≈Çu (mniej przydatny)
                text = self._extract_text_from_docx_bytes(file_bytes)
                return text, b""

        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd pobierania/parsowania pliku {url}: {e}")
            return "", b""

        return "", b""

    def _extract_text_from_pdf_bytes(self, pdf_bytes: bytes) -> str:
        """
        Ekstraktuje tekst z PDF z bajt√≥w
        """
        if not PDF_SUPPORT:
            self.logger.error("Brak wsparcia dla PDF - zainstaluj: pip install pdfplumber")
            return ""

        try:
            import io

            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                text = ""
                total_pages = len(pdf.pages)

                self.logger.debug(f"PDF ma {total_pages} stron")

                # Najpierw spr√≥buj standardowego wyciƒÖgania tekstu
                pages_with_text = 0
                for i, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text and len(page_text.strip()) > 50:  # Sensowna ilo≈õƒá tekstu
                            text += page_text + "\n"
                            pages_with_text += 1

                        # Log co 25 stron
                        if (i + 1) % 25 == 0:
                            self.logger.debug(
                                f"Przetworzono {i + 1}/{total_pages} stron, tekst na {pages_with_text} stronach")

                    except Exception as e:
                        self.logger.warning(f"B≈ÇƒÖd parsowania strony {i + 1}: {e}")
                        continue

                cleaned_text = text.strip()

                # Je≈õli brak tekstu, mo≈ºe to zeskanowany PDF - spr√≥buj OCR
                if len(cleaned_text) < 100 and total_pages > 5:
                    self.logger.warning(f"PDF ma {total_pages} stron ale tylko {len(cleaned_text)} znak√≥w tekstu")
                    self.logger.info("üîç Prawdopodobnie zeskanowany PDF - pr√≥bujƒô OCR...")

                    ocr_text = self._try_ocr_pdf(pdf_bytes)
                    if ocr_text and len(ocr_text) > len(cleaned_text):
                        self.logger.info(f"‚úÖ OCR wydoby≈Ç {len(ocr_text)} znak√≥w")
                        return ocr_text
                    else:
                        self.logger.warning("‚ùå OCR nie pom√≥g≈Ç lub nie jest dostƒôpny")

                if pages_with_text > 0:
                    self.logger.info(
                        f"üìÑ Wydobyto {len(cleaned_text)} znak√≥w z PDF ({pages_with_text}/{total_pages} stron z tekstem)")
                else:
                    self.logger.warning(f"‚ö†Ô∏è  PDF ma {total_pages} stron ale brak czytelnego tekstu")

                return cleaned_text

        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd parsowania PDF z bajt√≥w: {e}")
            return ""

    def _try_ocr_pdf(self, pdf_bytes: bytes) -> str:
        """
        Pr√≥buje OCR na PDF (wymaga tesseract + pdf2image)
        """
        try:
            # Sprawd≈∫ czy sƒÖ dostƒôpne biblioteki OCR
            try:
                from pdf2image import convert_from_bytes
                import pytesseract
            except ImportError:
                self.logger.debug("Brak bibliotek OCR (pdf2image, pytesseract)")
                return ""

            self.logger.info("üîç Rozpoczynam OCR PDF...")

            # Konwertuj PDF na obrazy (tylko pierwsze 10 stron dla testu)
            images = convert_from_bytes(pdf_bytes, first_page=1, last_page=10)

            ocr_text = ""
            for i, image in enumerate(images):
                try:
                    # OCR na ka≈ºdym obrazie
                    page_text = pytesseract.image_to_string(image, lang='pol+eng')
                    if page_text and len(page_text.strip()) > 20:
                        ocr_text += page_text + "\n"

                    self.logger.debug(f"OCR strona {i + 1}: {len(page_text)} znak√≥w")

                except Exception as e:
                    self.logger.warning(f"B≈ÇƒÖd OCR strony {i + 1}: {e}")
                    continue

            return ocr_text.strip()

        except Exception as e:
            self.logger.warning(f"B≈ÇƒÖd OCR: {e}")
            return ""

    def _extract_text_from_pdf(self, file_path: Path) -> str:
        """Ekstraktuje tekst z PDF"""
        if not PDF_SUPPORT:
            self.logger.error("Brak wsparcia dla PDF - zainstaluj pdfplumber")
            return ""

        try:
            with pdfplumber.open(file_path) as pdf:
                text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                return text.strip()
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd parsowania PDF {file_path}: {e}")
            return ""

    def _extract_text_from_docx(self, file_path: Path) -> str:
        """Ekstraktuje tekst z DOCX"""
        if not DOCX_SUPPORT:
            self.logger.error("Brak wsparcia dla DOCX - zainstaluj docx2txt")
            return ""

        try:
            return docx2txt.process(str(file_path))
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd parsowania DOCX {file_path}: {e}")
            return ""

    def _download_file(self, url: str, file_path: Path) -> bool:
        """Pobiera plik z URL"""
        try:
            response = self._make_request(url, stream=True)
            if not response:
                return False

            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            self.logger.info(f"Pobrano plik: {file_path}")
            return True

        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd pobierania pliku {url}: {e}")
            return False

    def _find_session_links(self, base_url: str) -> List[Dict[str, str]]:
        """Znajduje linki do posiedze≈Ñ na stronie g≈Ç√≥wnej"""
        sessions = []

        # Aktualne URLe dla r√≥≈ºnych kadencji
        search_urls = []

        if "sejm10" in base_url:
            search_urls = [
                f"{base_url}stenogramy.xsp",
                f"{base_url}",
                "https://orka2.sejm.gov.pl/StenogramyX.nsf/main",
            ]
        elif "sejm9" in base_url:
            search_urls = [
                f"{base_url}stenogramy.xsp",
                f"{base_url}",
            ]
        else:
            search_urls = [
                f"{base_url}stenogramy.xsp",
                f"{base_url}terminarz.xsp",
                f"{base_url}",
            ]

        for search_url in search_urls:
            self.logger.info(f"Sprawdzam URL: {search_url}")

            time.sleep(3)

            response = self._make_request(search_url)
            if not response:
                continue

            try:
                soup = BeautifulSoup(response.text, 'html.parser')

                # DEBUG: poka≈º co jest na stronie
                # todo wlƒÖcz mnie jak chcesz
                # if self.logger.level <= logging.DEBUG:
                #     self._debug_page_content(search_url, soup)

                # Rozszerzone wzorce wyszukiwania - DODANO NOWE
                link_patterns = [
                    # SPECJALNE dla sejm.gov.pl
                    {'selector': 'a.pdf', 'type': 'pdf_direct'},  # class="pdf"
                    {'selector': 'a[href*="ksiazka.pdf"]', 'type': 'ksiazka'},  # bezpo≈õrednie PDFy
                    {'selector': 'a[href*="StenoInter"]', 'type': 'stenointr'},  # serwer stenogram√≥w
                    {'selector': 'a[href*=".pdf"]', 'type': 'pdf'},  # wszystkie PDFy

                    # Klasyczne wzorce
                    {'selector': 'a[href*="stenogram"]', 'type': 'stenogram'},
                    {'selector': 'a[href*="Stenogram"]', 'type': 'stenogram'},
                    {'selector': 'a[href*="posiedzenie"]', 'type': 'posiedzenie'},
                    {'selector': 'a[href*="Posiedzenie"]', 'type': 'posiedzenie'},
                    {'selector': 'a[href*="nr"]', 'type': 'numer'},

                    # Ostateczny fallback
                    {'selector': 'a', 'type': 'general'}
                ]

                found_links = []

                for pattern in link_patterns:
                    links = soup.select(pattern['selector'])

                    self.logger.debug(f"Wzorzec '{pattern['selector']}' znalaz≈Ç {len(links)} link√≥w")

                    for link in links:
                        href = link.get('href', '')
                        text = link.get_text(strip=True)

                        if not href or not text or len(text.strip()) < 3:
                            continue

                        # Dla wzorc√≥w PDF/stenogram - mniej restrykcyjne sprawdzanie
                        if pattern['type'] in ['pdf_direct', 'ksiazka', 'stenointr', 'pdf']:
                            if self._is_pdf_session_link(href, text):
                                full_url = urljoin(search_url, href)
                                found_links.append((full_url, text))
                                self.logger.debug(f"PDF link znaleziony: {text[:50]}... -> {full_url}")

                        # Dla pozosta≈Çych - standardowe sprawdzanie
                        elif self._is_session_link(href, text):
                            full_url = urljoin(search_url, href)
                            found_links.append((full_url, text))
                            self.logger.debug(f"Session link znaleziony: {text[:50]}... -> {full_url}")

                    # Je≈õli znale≈∫li≈õmy linki PDF, sko≈Ñcz szukanie
                    if found_links and pattern['type'] in ['pdf_direct', 'ksiazka', 'stenointr']:
                        self.logger.info(f"Znaleziono {len(found_links)} link√≥w PDF, pomijam dalsze wzorce")
                        break

                # Przetw√≥rz znalezione linki
                for full_url, text in found_links:
                    session_number = self._extract_session_number(text)
                    session_id = hashlib.md5(full_url.encode()).hexdigest()[:12]
                    date_found = self._extract_date(text) or self._try_extract_date_from_url(full_url)

                    sessions.append({
                        'id': session_id,
                        'number': session_number if session_number > 0 else len(sessions) + 1,
                        'title': text[:200],
                        'url': full_url,
                        'date': date_found or datetime.now().strftime('%Y-%m-%d')
                    })

                self.logger.info(
                    f"Z {search_url} znaleziono {len([s for s in sessions if s['url'].startswith(search_url) or search_url in s['url']])} link√≥w")

            except Exception as e:
                self.logger.error(f"B≈ÇƒÖd parsowania {search_url}: {e}")
                import traceback
                self.logger.debug(f"Pe≈Çny b≈ÇƒÖd: {traceback.format_exc()}")

        # Usu≈Ñ duplikaty na podstawie URL
        unique_sessions = {}
        for session in sessions:
            unique_sessions[session['url']] = session

        sessions = list(unique_sessions.values())

        # Sortuj wed≈Çug daty/numeru
        sessions.sort(key=lambda x: (x['date'], x['number']), reverse=True)

        self.logger.info(f"Znaleziono ≈ÇƒÖcznie {len(sessions)} unikalnych sesji")
        return sessions

    def _is_pdf_session_link(self, href: str, text: str) -> bool:
        """Sprawdza czy PDF link to stenogram posiedzenia"""
        href_lower = href.lower()
        text_lower = text.lower().strip()

        # Negatywne wzorce dla PDF√≥w (nie chcemy np. regulamin√≥w PDF)
        negative_patterns = [
            'regulamin', 'procedura', 'instrukcja', 'menu', 'nav',
            'statut', 'zarzƒÖdzenie', 'uchwa≈Ça', 'regulacja'
        ]

        for pattern in negative_patterns:
            if pattern in href_lower or pattern in text_lower:
                return False

        # Pozytywne wzorce dla PDF√≥w stenogram√≥w
        if 'ksiazka.pdf' in href_lower:  # ksiazka.pdf = stenogram
            return True

        if 'stenointr' in href_lower or 'stenogram' in href_lower:  # serwer stenogram√≥w
            return True

        # Sprawd≈∫ czy tekst wyglƒÖda jak data posiedzenia
        if self._looks_like_date(text):
            return True

        # Sprawd≈∫ wzorce numeryczne
        import re
        if re.search(r'posiedzenie.*\d+', text_lower) or re.search(r'nr.*\d+', text_lower):
            return True

        return False

    def _should_skip_session(self, session_title: str) -> bool:
        """
        Sprawdza czy sesjƒô nale≈ºy pominƒÖƒá (fotogalerie, zapowiedzi itp.)
        """
        title_lower = session_title.lower()

        # Wzorce do pomijania
        skip_patterns = [
            '[fotogaleria]',
            'fotogaleria',
            '(zapowied≈∫)',
            'zapowied≈∫',
            'spotkanie marsza≈Çka',
            'spotkanie wicemarsza≈Çek',
            'udzia≈Ç marsza≈Çka',
            'galeria zdjƒôƒá',
            'zdjƒôcia z',
            'foto:',
            'fotorelacja',
            'relacja fotograficzna'
        ]

        # Sprawd≈∫ czy tytu≈Ç zawiera kt√≥ry≈õ ze wzorc√≥w
        for pattern in skip_patterns:
            if pattern in title_lower:
                self.logger.info(f"‚è≠Ô∏è  Pominam sesjƒô (fotogaleria/zapowied≈∫): {session_title[:50]}...")
                return True

        # Sprawd≈∫ czy to bardzo kr√≥tki tekst (prawdopodobnie nie stenogram)
        if len(session_title.strip()) < 10:
            self.logger.info(f"‚è≠Ô∏è  Pominam sesjƒô (za kr√≥tki tytu≈Ç): {session_title}")
            return True

        return False

    def _extract_session_number(self, text: str) -> int:
        """WyciƒÖga numer posiedzenia z tekstu"""
        # Szuka wzorc√≥w jak "Nr 15", "posiedzenie 23", "sesja 5"
        patterns = [
            r'nr\s*(\d+)',
            r'posiedzenie\s*(\d+)',
            r'sesja\s*(\d+)',
            r'(\d+)\s*posiedzenie',
            r'(\d+)\.'
        ]

        for pattern in patterns:
            match = re.search(pattern, text.lower())
            if match:
                return int(match.group(1))

        return 0  # Je≈õli nie znaleziono numeru

    def _extract_date(self, text: str) -> Optional[str]:
        """WyciƒÖga datƒô z tekstu"""
        # Wzorce polskich dat
        date_patterns = [
            r'(\d{1,2})\s*(stycznia|lutego|marca|kwietnia|maja|czerwca|lipca|sierpnia|wrze≈õnia|pa≈∫dziernika|listopada|grudnia)\s*(\d{4})',
            r'(\d{1,2})\.(\d{1,2})\.(\d{4})',
            r'(\d{4})-(\d{1,2})-(\d{1,2})'
        ]

        polish_months = {
            'stycznia': '01', 'lutego': '02', 'marca': '03', 'kwietnia': '04',
            'maja': '05', 'czerwca': '06', 'lipca': '07', 'sierpnia': '08',
            'wrze≈õnia': '09', 'pa≈∫dziernika': '10', 'listopada': '11', 'grudnia': '12'
        }

        for pattern in date_patterns:
            match = re.search(pattern, text.lower())
            if match:
                try:
                    if 'stycznia' in pattern:  # wzorzec polski
                        day, month_name, year = match.groups()
                        month = polish_months.get(month_name, '01')
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    elif '.' in pattern:  # DD.MM.YYYY
                        day, month, year = match.groups()
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    else:  # YYYY-MM-DD
                        return match.group(0)
                except:
                    continue

        return None

    def _process_session(self, session_data: Dict[str, str]) -> Optional[SejmSession]:
        """
        Przetwarza pojedynczƒÖ sesjƒô z filtrowaniem i PDF
        """
        session_id = session_data['id']

        if session_id in self.processed_sessions:
            self.logger.debug(f"Sesja {session_id} ju≈º przetworzona, pomijam")
            return None

        # NOWE: Sprawd≈∫ czy to nie fotogaleria/zapowied≈∫
        if self._should_skip_session(session_data['title']):
            return None

        self.logger.info(f"Przetwarzam sesjƒô: {session_data['title']}")

        # Pobiera stronƒô sesji
        response = self._make_request(session_data['url'])
        if not response:
            return None

        # Sprawd≈∫ czy to bezpo≈õredni link do PDF
        content_type = response.headers.get('content-type', '').lower()
        is_direct_pdf = 'application/pdf' in content_type or session_data['url'].endswith('.pdf')

        transcript_url = None
        file_type = 'html'

        if is_direct_pdf:
            # To jest bezpo≈õredni PDF
            transcript_url = session_data['url']
            file_type = 'pdf'
            self.logger.info(f"Wykryto bezpo≈õredni PDF: {transcript_url}")
        else:
            # Pr√≥buje znale≈∫ƒá link do transkryptu na stronie HTML
            soup = BeautifulSoup(response.text, 'html.parser')

            # Szuka link√≥w do plik√≥w - POPRAWIONE WZORCE
            pdf_patterns = [
                'a[href*=".pdf"]',
                'a[href*="ksiazka"]',
                'a.pdf',
                'a[class*="pdf"]'
            ]

            for pattern in pdf_patterns:
                links = soup.select(pattern)
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True).lower()

                    # Sprawd≈∫ czy to stenogram
                    if any(word in text for word in ['stenogram', 'posiedzenie', 'ksiƒôga', 'transkryp']):
                        transcript_url = urljoin(session_data['url'], href)
                        file_type = 'pdf' if href.endswith('.pdf') else 'html'
                        self.logger.info(f"Znaleziono link do PDF: {transcript_url}")
                        break

                if transcript_url:
                    break

            # Je≈õli nie znaleziono PDF, sprawd≈∫ tradycyjne wzorce HTML
            if not transcript_url:
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    text = link.get_text(strip=True).lower()

                    if any(pattern in text for pattern in self.config.transcript_patterns):
                        if href.endswith('.pdf'):
                            file_type = 'pdf'
                        elif href.endswith('.docx') or href.endswith('.doc'):
                            file_type = 'docx'

                        transcript_url = urljoin(session_data['url'], href)
                        break

        # Je≈õli nie znaleziono osobnego linku, u≈ºyj tej samej strony
        if not transcript_url:
            transcript_url = session_data['url']
            file_type = 'html'

        # Tworzy obiekt sesji
        session = SejmSession(
            session_id=session_id,
            session_number=session_data['number'],
            date=session_data['date'],
            title=session_data['title'],
            url=session_data['url'],
            transcript_url=transcript_url,
            file_type=file_type,
            scraped_at=datetime.now().isoformat()
        )

        # Pobiera i parsuje tre≈õƒá + zachowuje PDF
        text_content = ""
        pdf_bytes = b""

        try:
            if file_type == 'html':
                # HTML
                if transcript_url == session_data['url']:
                    text_content = self._extract_text_from_html(response.text)
                else:
                    transcript_response = self._make_request(transcript_url)
                    if transcript_response:
                        text_content = self._extract_text_from_html(transcript_response.text)

            elif file_type in ['pdf', 'docx']:
                # NOWE: Pobiera plik i zwraca tekst + oryginalne bajty
                text_content, pdf_bytes = self._download_and_parse_file(
                    transcript_url, file_type, session_id
                )

        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd pobierania tre≈õci dla sesji {session_id}: {e}")
            return None

        # Sprawd≈∫ czy mamy sensownƒÖ tre≈õƒá
        if text_content and len(text_content.strip()) > 100:
            session.transcript_text = text_content
            session.hash = hashlib.md5(text_content.encode()).hexdigest()

            # NOWE: Zapisz w obu formatach
            if self._save_session_with_pdf(session, pdf_bytes):
                self._save_processed_session(session_id)

                self.logger.info(f"‚úÖ Pomy≈õlnie przetworzono sesjƒô {session_id}, tekst: {len(text_content)} znak√≥w")
                return session
            else:
                self.logger.error(f"‚ùå B≈ÇƒÖd zapisu sesji {session_id}")
                return None
        else:
            self.logger.warning(f"‚ö†Ô∏è  Brak tre≈õci lub tre≈õƒá za kr√≥tka dla sesji {session_id}")
            return None

    def _save_session(self, session: SejmSession):
        """Zapisuje sesjƒô do pliku z walidacjƒÖ"""
        year = session.date[:4] if session.date and len(session.date) >= 4 else str(date.today().year)
        year_dir = self.config.output_dir / year
        year_dir.mkdir(exist_ok=True)

        filename = f"posiedzenie_{session.session_number:03d}_{session.session_id}.json"
        filepath = year_dir / filename

        try:
            # Przygotuj dane do zapisu - BEZ surowych danych PDF!
            session_data = asdict(session)

            # Dodaj metadane
            session_data['text_length'] = len(session.transcript_text) if session.transcript_text else 0
            session_data['word_count'] = len(session.transcript_text.split()) if session.transcript_text else 0

            # Sprawd≈∫ czy tekst nie jest binarny (zabezpieczenie)
            if session.transcript_text:
                try:
                    # Pr√≥ba zakodowania - je≈õli siƒô nie uda, tekst mo≈ºe byƒá uszkodzony
                    session.transcript_text.encode('utf-8')
                except UnicodeEncodeError:
                    self.logger.error(f"Tekst sesji {session.session_id} zawiera nieprawid≈Çowe znaki!")
                    session_data['transcript_text'] = "[B≈ÅƒÑD: Nieprawid≈Çowe znaki w tek≈õcie]"

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"üíæ Zapisano sesjƒô: {filepath} ({session_data['text_length']} znak√≥w)")

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd zapisu sesji {session.session_id}: {e}")
            import traceback
            self.logger.debug(f"Pe≈Çny b≈ÇƒÖd zapisu: {traceback.format_exc()}")

    def _save_session_with_pdf(self, session: SejmSession, pdf_bytes: bytes = None):
        """
        Zapisuje sesjƒô w JSON + opcjonalnie oryginalny PDF
        """
        year = session.date[:4] if session.date and len(session.date) >= 4 else str(date.today().year)
        year_dir = self.config.output_dir / year
        year_dir.mkdir(exist_ok=True)

        # Katalogi dla r√≥≈ºnych format√≥w
        json_dir = year_dir / "json"
        pdf_dir = year_dir / "pdf"
        json_dir.mkdir(exist_ok=True)
        pdf_dir.mkdir(exist_ok=True)

        base_filename = f"posiedzenie_{session.session_number:03d}_{session.session_id}"

        # 1. Zapisz JSON (jak dotychczas)
        json_filepath = json_dir / f"{base_filename}.json"

        try:
            session_data = asdict(session)
            session_data['text_length'] = len(session.transcript_text) if session.transcript_text else 0
            session_data['word_count'] = len(session.transcript_text.split()) if session.transcript_text else 0

            # Dodaj info o PDF
            if pdf_bytes:
                session_data['original_pdf_available'] = True
                session_data['pdf_size_bytes'] = len(pdf_bytes)
            else:
                session_data['original_pdf_available'] = False

            with open(json_filepath, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"üíæ JSON: {json_filepath} ({session_data['text_length']} znak√≥w)")

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd zapisu JSON {session.session_id}: {e}")
            return False

        # 2. Zapisz oryginalny PDF (je≈õli dostƒôpny)
        if pdf_bytes and len(pdf_bytes) > 1000:  # Sprawd≈∫ czy to sensowny PDF
            pdf_filepath = pdf_dir / f"{base_filename}.pdf"

            try:
                with open(pdf_filepath, 'wb') as f:
                    f.write(pdf_bytes)

                self.logger.info(f"üìÑ PDF: {pdf_filepath} ({len(pdf_bytes)} bajt√≥w)")

            except Exception as e:
                self.logger.error(f"‚ùå B≈ÇƒÖd zapisu PDF {session.session_id}: {e}")

        return True

    def cleanup_broken_sessions(self):
        """Czy≈õci uszkodzone pliki JSON z surowym PDF"""
        broken_count = 0

        for json_file in self.config.output_dir.rglob("*.json"):
            if json_file.name == "processed_sessions.json":
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # Sprawd≈∫ czy zawiera binarne dane PDF
                transcript = data.get('transcript_text', '')
                if transcript and ('%PDF-' in transcript or '\u0000' in transcript):
                    self.logger.info(f"üßπ Usuwam uszkodzony plik: {json_file}")
                    json_file.unlink()

                    # Usu≈Ñ z processed_sessions ≈ºeby ponownie przetworzyƒá
                    session_id = data.get('session_id')
                    if session_id in self.processed_sessions:
                        self.processed_sessions.remove(session_id)

                    broken_count += 1

            except Exception as e:
                self.logger.warning(f"Nie mo≈ºna sprawdziƒá pliku {json_file}: {e}")

        if broken_count > 0:
            # Zapisz zaktualizowanƒÖ listƒô processed_sessions
            self._save_processed_session("")  # Zapisuje ca≈ÇƒÖ listƒô
            self.logger.info(f"üßπ Wyczyszczono {broken_count} uszkodzonych plik√≥w")

        return broken_count

    def run(self) -> int:
        """G≈Ç√≥wna pƒôtla bota"""
        print("üèõÔ∏è  SejmBot - Parser transkrypt√≥w Sejmu RP")
        print("=" * 50)
        self.logger.info("üöÄ Uruchomiono SejmBot")

        total_processed = 0

        for base_url in self.config.base_urls:
            self.logger.info(f"Przeszukujƒô: {base_url}")

            # Znajduje linki do sesji
            session_links = self._find_session_links(base_url)

            for session_data in session_links:
                try:
                    result = self._process_session(session_data)
                    if result:
                        total_processed += 1
                        self.logger.info(f"‚úÖ Przetworzona sesja: {result.title}")

                except Exception as e:
                    self.logger.error(f"B≈ÇƒÖd przetwarzania sesji {session_data.get('title', 'unknown')}: {e}")

        self.logger.info(f"üéâ Zako≈Ñczono. Przetworzono {total_processed} nowych sesji")
        return total_processed


def main():
    """Punkt wej≈õcia programu"""

    # Inicjalizacja
    config = SejmBotConfig()
    bot = SejmBot(config)

    try:
        # Uruchomienie
        processed_count = bot.run()

        if processed_count > 0:
            print(f"\n‚úÖ Sukces! Przetworzono {processed_count} nowych transkrypt√≥w")
        else:
            print("\nüìã Brak nowych transkrypt√≥w do przetworzenia")

    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Przerwano przez u≈ºytkownika")
    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd krytyczny: {e}")
        logging.error(f"B≈ÇƒÖd krytyczny: {e}", exc_info=True)


if __name__ == "__main__":
    main()
