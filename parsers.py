#!/usr/bin/env python3
"""
SejmBot - parsers.py
Modu≈Çy do parsowania i ekstrakcji tekstu z r√≥≈ºnych format√≥w dokument√≥w
"""

import io
import logging
import os
import re
import tempfile

from bs4 import BeautifulSoup

# Importy opcjonalne - sprawdzamy dostƒôpno≈õƒá bibliotek
try:
    import pdfplumber

    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

try:
    import docx2txt

    DOCX_SUPPORT = True
except ImportError:
    DOCX_SUPPORT = False


class HTMLParser:
    """Parser dla stron HTML z transkryptami"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger

    def extract_text(self, html_content: str) -> str:
        """Ekstraktuje tekst z HTML"""
        if not html_content:
            return ""

        soup = BeautifulSoup(html_content, 'html.parser')

        # Usuwa zbƒôdne elementy
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()

        # Szuka g≈Ç√≥wnej tre≈õci oraz wzorc√≥w dla stron Sejmu
        content_selectors = [
            # Specjalne dla Sejmu
            'div[id*="stenogram"]',
            'div[class*="stenogram"]',
            'div[class*="transcript"]',
            'div[class*="protokol"]',
            'div[id*="protokol"]',
            '.stenogram-content',
            '.transcript-content',

            # Og√≥lne wzorce
            'main',
            'article',
            '.main-content',
            '.content',
            '#content',
            '.post-content',

            # Fallback - ca≈Ça tre≈õƒá body bez nav/header/footer
            'body'
        ]

        text_content = ""
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                # We≈∫ pierwszy znaleziony element
                element = elements[0]
                text_content = element.get_text(separator=' ', strip=True)
                if len(text_content) > 200:  # Sprawd≈∫ czy to sensowna tre≈õƒá
                    self.logger.debug(f"U≈ºyto selektora: {selector}, tekst: {len(text_content)} znak√≥w")
                    break

        # Je≈õli nadal brak tre≈õci, we≈∫ ca≈Çy tekst
        if not text_content or len(text_content) < 200:
            text_content = soup.get_text(separator=' ', strip=True)

        # Czyszczenie tekstu
        cleaned_text = TextCleaner.clean_extracted_text(text_content)

        self.logger.debug(f"HTML -> tekst: {len(cleaned_text)} znak√≥w")
        return cleaned_text


class PDFParser:
    """Parser dla dokument√≥w PDF"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger

        if not PDF_SUPPORT:
            self.logger.warning("‚ö†Ô∏è  Brak wsparcia dla PDF - zainstaluj: pip install pdfplumber")

    def extract_text_from_bytes(self, pdf_bytes: bytes) -> str:
        """Ekstraktuje tekst z PDF z bajt√≥w z obs≈ÇugƒÖ r√≥≈ºnych problem√≥w"""
        if not PDF_SUPPORT:
            self.logger.error("‚ùå Brak wsparcia dla PDF - zainstaluj: pip install pdfplumber")
            return ""

        if not pdf_bytes or len(pdf_bytes) < 100:
            self.logger.error("‚ùå PDF jest pusty lub uszkodzony")
            return ""

        try:
            # Sprawd≈∫ czy to rzeczywi≈õcie PDF
            if not pdf_bytes.startswith(b'%PDF-'):
                self.logger.error("‚ùå Plik nie jest prawid≈Çowym PDF")
                return ""

            pdf_stream = io.BytesIO(pdf_bytes)
            text_parts = []

            with pdfplumber.open(pdf_stream) as pdf:
                self.logger.info(f"üìÑ PDF ma {len(pdf.pages)} stron")

                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(page_text)

                        # Poka≈º postƒôp dla du≈ºych PDF√≥w
                        if page_num % 50 == 0:
                            self.logger.info(f"üìñ Przetworzono {page_num}/{len(pdf.pages)} stron")

                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è  B≈ÇƒÖd na stronie {page_num}: {e}")
                        continue

            full_text = '\n\n'.join(text_parts)
            cleaned_text = TextCleaner.clean_extracted_text(full_text)

            self.logger.info(f"‚úÖ WyciƒÖgniƒôto {len(cleaned_text):,} znak√≥w z PDF")
            return cleaned_text

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd parsowania PDF: {e}")
            return ""


class DOCXParser:
    """Parser dla dokument√≥w DOCX"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger

        if not DOCX_SUPPORT:
            self.logger.warning("‚ö†Ô∏è  Brak wsparcia dla DOCX - zainstaluj: pip install docx2txt")

    def extract_text_from_bytes(self, docx_bytes: bytes) -> str:
        """Ekstraktuje tekst z DOCX z bajt√≥w"""
        if not DOCX_SUPPORT:
            self.logger.error("‚ùå Brak wsparcia dla DOCX - zainstaluj: pip install docx2txt")
            return ""

        try:
            # docx2txt wymaga pliku na dysku
            with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as tmp_file:
                tmp_file.write(docx_bytes)
                tmp_file_path = tmp_file.name

            try:
                text = docx2txt.process(tmp_file_path)
                cleaned_text = TextCleaner.clean_extracted_text(text) if text else ""

                self.logger.info(f"‚úÖ WyciƒÖgniƒôto {len(cleaned_text):,} znak√≥w z DOCX")
                return cleaned_text

            finally:
                # Usu≈Ñ plik tymczasowy
                try:
                    os.unlink(tmp_file_path)
                except:
                    pass

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd parsowania DOCX z bajt√≥w: {e}")
            return ""

    def extract_text_from_file(self, file_path: str) -> str:
        """Ekstraktuje tekst z pliku DOCX na dysku"""
        if not DOCX_SUPPORT:
            self.logger.error("‚ùå Brak wsparcia dla DOCX - zainstaluj docx2txt")
            return ""

        try:
            text = docx2txt.process(file_path)
            cleaned_text = TextCleaner.clean_extracted_text(text) if text else ""

            self.logger.info(f"‚úÖ WyciƒÖgniƒôto {len(cleaned_text):,} znak√≥w z DOCX")
            return cleaned_text

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd parsowania DOCX {file_path}: {e}")
            return ""


class TextCleaner:
    """Klasa statyczna do czyszczenia i normalizacji tekstu"""

    @staticmethod
    def clean_extracted_text(text: str) -> str:
        """
        Czy≈õci wyciƒÖgniƒôty tekst z PDF/HTML/DOCX
        """
        if not text:
            return ""

        # Usu≈Ñ nadmiarowe bia≈Çe znaki
        text = re.sub(r'\s+', ' ', text)

        # Usu≈Ñ typowe ≈õmieci z dokument√≥w
        cleanup_patterns = [
            r'JavaScript.*?w≈ÇƒÖcz',
            r'Cookie.*?polityka',
            r'Mapa strony',
            r'Menu g≈Ç√≥wne',
            r'Nawigacja',
            r'Przejd≈∫ do',
            r'Skip to',
            r'Strona \d+ z \d+',  # Numery stron
            r'www\.sejm\.gov\.pl',
            r'¬© Kancelaria Sejmu',
        ]

        for pattern in cleanup_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # Normalizuj polskie znaki i typografiƒô
        text = text.replace('\u00a0', ' ')  # non-breaking space
        text = text.replace('\u2013', '-')  # en dash
        text = text.replace('\u2014', '--')  # em dash
        text = text.replace('\u2019', "'")  # right single quotation mark
        text = text.replace('\u201c', '"')  # left double quotation mark
        text = text.replace('\u201d', '"')  # right double quotation mark

        # Ko≈Ñcowe czyszczenie
        text = text.strip()
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # Usu≈Ñ nadmierne puste linie

        return text

    @staticmethod
    def normalize_polish_text(text: str) -> str:
        """Normalizuje polski tekst dla lepszej analizy"""
        if not text:
            return ""

        # Zamie≈Ñ typowe skr√≥ty parlamentarne na pe≈Çne formy
        replacements = {
            r'\bp\.\s*pos\.': 'pan pose≈Ç',
            r'\bp\.\s*pos\.\s*': 'pani pos≈Çanka',
            r'\bmin\.': 'minister',
            r'\bmarsza≈Çek\s+sejmu': 'marsza≈Çek Sejmu',
            r'\bwicemarsza≈Çek': 'wicemarsza≈Çek',
            r'\br\.m\.': 'rada ministr√≥w',
        }

        for pattern, replacement in replacements.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

        return text


class TextValidator:
    """Walidator jako≈õci wyciƒÖgniƒôtego tekstu"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger

    def validate_stenogram_text(self, text: str, source_url: str = "") -> bool:
        """
        Waliduje czy wyciƒÖgniƒôty tekst jest sensowny stenogram Sejmu
        """
        if not text or len(text.strip()) < 500:  # Zwiƒôkszony pr√≥g
            self.logger.warning(f"‚ö†Ô∏è  Tekst za kr√≥tki ({len(text)} znak√≥w) dla {source_url}")
            return False

        # Sprawd≈∫ czy tekst zawiera polskie s≈Çowa charakterystyczne dla stenogram√≥w
        polish_keywords = [
            'posiedzenie', 'marsza≈Çek', 'pose≈Ç', 'pos≈Çanka', 'sejm', 'g≈Çosowanie',
            'komisja', 'sprawozdanie', 'ustawa', 'interpelacja', 'punkt', 'porzƒÖdku',
            'obrady', 'wicemarsza≈Çek', 'przewodniczƒÖcy', 'sekretarz', 'protok√≥≈Ç',
            'rada ministr√≥w', 'rzƒÖd', 'minister', 'klub', 'ko≈Ço', 'poselski'
        ]

        text_lower = text.lower()
        found_keywords = sum(1 for keyword in polish_keywords if keyword in text_lower)

        if found_keywords < 5:  # Zwiƒôkszony pr√≥g
            self.logger.warning(
                f"‚ö†Ô∏è  Tekst mo≈ºe nie byƒá stenogramem - znaleziono tylko {found_keywords} kluczowych s≈Ç√≥w"
            )
            return False

        # Sprawd≈∫ czy nie jest to g≈Ç√≥wnie ≈õmieci/kod
        printable_ratio = sum(1 for c in text if c.isprintable() or c.isspace()) / len(text) if text else 0

        if printable_ratio < 0.9:  # Zwiƒôkszony pr√≥g jako≈õci
            self.logger.warning(f"‚ö†Ô∏è  Tekst zawiera du≈ºo nieprintable znak√≥w ({printable_ratio:.2%})")
            return False

        # Sprawd≈∫ czy tekst nie sk≈Çada siƒô g≈Ç√≥wnie z pojedynczych liter/cyfr
        words = text.split()
        meaningful_words = sum(1 for word in words if len(word) > 2)
        meaningful_ratio = meaningful_words / len(words) if words else 0

        if meaningful_ratio < 0.7:
            self.logger.warning(f"‚ö†Ô∏è  Tekst ma za ma≈Ço sensownych s≈Ç√≥w ({meaningful_ratio:.2%})")
            return False

        # Sprawd≈∫ typowe b≈Çƒôdy OCR/parsingu
        suspicious_patterns = [
            r'^[^a-zƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]*$',  # Brak polskich liter w ca≈Çym tek≈õcie
            r'^\d+\s*$',  # Tylko cyfry
            r'^[^\w\s]*$',  # Tylko znaki specjalne
        ]

        for pattern in suspicious_patterns:
            if re.match(pattern, text_lower):
                self.logger.warning(f"‚ö†Ô∏è  Tekst pasuje do podejrzanego wzorca: {pattern}")
                return False

        self.logger.info(
            f"‚úÖ Tekst przeszed≈Ç walidacjƒô: {len(text):,} znak√≥w, "
            f"{found_keywords} kluczowych s≈Ç√≥w, {meaningful_ratio:.1%} sensownych s≈Ç√≥w"
        )
        return True

    def detect_encoding_issues(self, text: str) -> bool:
        """Wykrywa problemy z kodowaniem tekstu"""
        if not text:
            return False

        # Sprawd≈∫ problematyczne znaki
        problematic_chars = [
            '\ufffd',  # replacement character
            '\x00',  # null bytes
            '\x01',  # control characters
        ]

        for char in problematic_chars:
            if char in text:
                self.logger.warning(f"‚ö†Ô∏è  Wykryto problematyczny znak w tek≈õcie: {repr(char)}")
                return True

        # Sprawd≈∫ czy mo≈ºna zakodowaƒá do UTF-8
        try:
            text.encode('utf-8')
            return False
        except UnicodeEncodeError as e:
            self.logger.warning(f"‚ö†Ô∏è  B≈ÇƒÖd kodowania UTF-8: {e}")
            return True


class ContentTypeDetector:
    """Wykrywa typ zawarto≈õci pliku"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger

    def detect_from_bytes(self, content_bytes: bytes, url: str = "", content_type: str = "") -> str:
        """
        Wykrywa typ pliku na podstawie jego zawarto≈õci
        Zwraca: 'pdf', 'docx', 'html', 'unknown'
        """
        if not content_bytes:
            return "unknown"

        # Sprawd≈∫ pierwsze bajty (magic numbers)
        if content_bytes.startswith(b'%PDF-'):
            self.logger.debug("Wykryto PDF na podstawie magic number")
            return "pdf"

        # DOCX to ZIP z okre≈õlonƒÖ strukturƒÖ
        if content_bytes.startswith(b'PK\x03\x04'):
            # Sprawd≈∫ czy to DOCX (zawiera word/ folder)
            try:
                if b'word/' in content_bytes[:2048]:
                    self.logger.debug("Wykryto DOCX na podstawie struktury ZIP")
                    return "docx"
            except:
                pass

        # HTML - sprawd≈∫ czy zawiera tagi HTML
        try:
            text_start = content_bytes[:2048].decode('utf-8', errors='ignore').lower()
            if any(tag in text_start for tag in ['<html', '<head', '<body', '<!doctype']):
                self.logger.debug("Wykryto HTML na podstawie tag√≥w")
                return "html"
        except:
            pass

        # Fallback na podstawie URL i content-type
        if url:
            if url.lower().endswith('.pdf'):
                return "pdf"
            elif url.lower().endswith(('.docx', '.doc')):
                return "docx"
            elif url.lower().endswith(('.html', '.htm')):
                return "html"

        if content_type:
            if 'pdf' in content_type.lower():
                return "pdf"
            elif 'officedocument' in content_type.lower() or 'docx' in content_type.lower():
                return "docx"
            elif 'html' in content_type.lower():
                return "html"

        self.logger.warning(f"‚ö†Ô∏è  Nie mo≈ºna wykryƒá typu pliku dla URL: {url[:60]}")
        return "unknown"


class UniversalParser:
    """Uniwersalny parser ≈ÇƒÖczƒÖcy wszystkie typy dokument√≥w"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.html_parser = HTMLParser(logger)
        self.pdf_parser = PDFParser(logger)
        self.docx_parser = DOCXParser(logger)
        self.detector = ContentTypeDetector(logger)
        self.validator = TextValidator(logger)

    def parse_content(self, content_bytes: bytes, url: str = "", content_type: str = "") -> tuple[str, str]:
        """
        Uniwersalne parsowanie zawarto≈õci
        Zwraca: (extracted_text, detected_file_type)
        """
        if not content_bytes:
            self.logger.error("‚ùå Brak danych do parsowania")
            return "", "unknown"

        # Wykryj typ pliku
        file_type = self.detector.detect_from_bytes(content_bytes, url, content_type)

        self.logger.info(f"üîç Wykryto typ pliku: {file_type} dla URL: {url[:60]}...")

        # Parsuj wed≈Çug typu
        text = ""

        if file_type == "pdf":
            text = self.pdf_parser.extract_text_from_bytes(content_bytes)

        elif file_type == "docx":
            text = self.docx_parser.extract_text_from_bytes(content_bytes)

        elif file_type == "html":
            try:
                html_content = content_bytes.decode('utf-8', errors='replace')
                text = self.html_parser.extract_text(html_content)
            except Exception as e:
                self.logger.error(f"‚ùå B≈ÇƒÖd dekodowania HTML: {e}")
                return "", file_type

        else:
            # Pr√≥ba dekodowania jako tekst
            try:
                text = content_bytes.decode('utf-8', errors='replace')
                text = TextCleaner.clean_extracted_text(text)
                self.logger.info("üìù Przetworzono jako zwyk≈Çy tekst")
            except Exception as e:
                self.logger.error(f"‚ùå Nie mo≈ºna zdekodowaƒá zawarto≈õci: {e}")
                return "", "unknown"

        # Walidacja jako≈õci tekstu
        if text and not self.validator.validate_stenogram_text(text, url):
            self.logger.warning(f"‚ö†Ô∏è  Tekst nie przeszed≈Ç walidacji jako≈õci")

        return text, file_type


class ParsingManager:
    """Manager zarzƒÖdzajƒÖcy wszystkimi parserami"""

    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.universal_parser = UniversalParser(logger)

        # Sprawd≈∫ dostƒôpno≈õƒá bibliotek
        self._check_dependencies()

    def _check_dependencies(self):
        """Sprawdza dostƒôpno≈õƒá bibliotek parsujƒÖcych"""
        missing = []

        if not PDF_SUPPORT:
            missing.append("pdfplumber (dla PDF)")

        if not DOCX_SUPPORT:
            missing.append("docx2txt (dla DOCX)")

        if missing:
            self.logger.warning(f"‚ö†Ô∏è  BrakujƒÖce biblioteki: {', '.join(missing)}")
            self.logger.info("üí° Zainstaluj: pip install pdfplumber docx2txt")
        else:
            self.logger.info("‚úÖ Wszystkie biblioteki parsujƒÖce dostƒôpne")

    def parse_file_content(self, content_bytes: bytes, url: str = "",
                           expected_type: str = "") -> tuple[str, str]:
        """
        G≈Ç√≥wna metoda parsowania - deleguje do odpowiedniego parsera

        Args:
            content_bytes: Zawarto≈õƒá pliku w bajtach
            url: URL ≈∫r√≥d≈Çowy (dla kontekstu)
            expected_type: Oczekiwany typ pliku ('pdf', 'docx', 'html')

        Returns:
            tuple[str, str]: (extracted_text, actual_file_type)
        """
        return self.universal_parser.parse_content(content_bytes, url, expected_type)

    def validate_text_quality(self, text: str, source_url: str = "") -> bool:
        """Waliduje jako≈õƒá wyciƒÖgniƒôtego tekstu"""
        return self.validator.validate_stenogram_text(text, source_url)

    def get_parsing_stats(self) -> dict:
        """Zwraca statystyki dostƒôpno≈õci parser√≥w"""
        return {
            'pdf_support': PDF_SUPPORT,
            'docx_support': DOCX_SUPPORT,
            'html_support': True,  # BeautifulSoup zawsze dostƒôpne
            'missing_dependencies': self._get_missing_dependencies()
        }

    def _get_missing_dependencies(self) -> list:
        """Zwraca listƒô brakujƒÖcych zale≈ºno≈õci"""
        missing = []

        if not PDF_SUPPORT:
            missing.append("pdfplumber")

        if not DOCX_SUPPORT:
            missing.append("docx2txt")

        return missing
