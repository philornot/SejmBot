#!/usr/bin/env python3
"""
SejmBot - downloader.py
Modu≈Ç do pobierania plik√≥w z internetu z obs≈ÇugƒÖ retry, alternative URLs i error handling
"""

import logging
import time
from typing import Optional, Tuple, List
from urllib.parse import urlparse

import requests
from requests.exceptions import RequestException, Timeout, ConnectionError


class FileDownloader:
    """Mened≈ºer pobierania plik√≥w z zaawansowanƒÖ obs≈ÇugƒÖ b≈Çƒôd√≥w"""

    def __init__(self, config, logger: logging.Logger):
        self.config = config
        self.logger = logger

        # Konfiguracja sesji HTTP
        self.session = requests.Session()
        self.session.headers.update(config.headers)

        # Dodaj User-Agent z konfiguracji
        self.session.headers['User-Agent'] = config.user_agent

    def download_file(self, url: str, **kwargs) -> Optional[requests.Response]:
        """
        Pobiera plik z obs≈ÇugƒÖ retry, alternative URLs i lepszej obs≈Çugi b≈Çƒôd√≥w

        Args:
            url: URL do pobrania
            **kwargs: Dodatkowe parametry dla requests

        Returns:
            requests.Response lub None w przypadku b≈Çƒôdu
        """
        # Przygotuj listƒô URLi do wypr√≥bowania
        urls_to_try = self._prepare_alternative_urls(url)

        # Ustaw timeout w zale≈ºno≈õci od serwera
        timeout = self._get_optimal_timeout(url)
        kwargs.setdefault('timeout', timeout)

        # Pr√≥buj ka≈ºdy URL z retry
        for url_to_try in urls_to_try:
            response = self._try_download_with_retry(url_to_try, **kwargs)
            if response:
                if url_to_try != url:
                    self.logger.info(f"‚úÖ Sukces z alternatywnym URL: {url_to_try}")
                return response

        self.logger.error(f"‚ùå Wszystkie pr√≥by pobierania nie powiod≈Çy siƒô dla: {url}")
        return None

    def download_and_parse_text(self, url: str, file_type: str, session_id: str) -> Tuple[str, bytes]:
        """
        Pobiera plik i zwraca zar√≥wno tekst jak i oryginalne bajty

        Args:
            url: URL do pobrania
            file_type: typ pliku ('pdf', 'docx', 'html')
            session_id: ID sesji dla logowania

        Returns:
            Tuple[tekst, bajty_pliku]
        """
        try:
            self.logger.info(f"üì• Pobieranie {file_type.upper()}: {url}")

            response = self.download_file(url)
            if not response:
                self.logger.error(f"‚ùå Nie uda≈Ço siƒô pobraƒá pliku: {url}")
                return "", b""

            # Pobierz oryginalne bajty
            file_bytes = response.content

            # Sprawd≈∫ rozmiar pliku
            file_size_mb = len(file_bytes) / (1024 * 1024)
            self.logger.info(f"üì¶ Rozmiar pliku: {file_size_mb:.2f} MB")

            # Sprawd≈∫ content-type
            content_type = response.headers.get('content-type', '').lower()
            self.logger.debug(f"Content-Type: {content_type}")

            # Parsuj zawarto≈õƒá w zale≈ºno≈õci od typu
            if file_type == 'pdf':
                return self._parse_pdf_bytes(file_bytes, url), file_bytes
            elif file_type == 'docx':
                return self._parse_docx_bytes(file_bytes, url), file_bytes
            else:  # HTML lub inne
                return self._parse_html_text(response.text), response.content

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd pobierania/parsowania pliku {url}: {e}")
            return "", b""

    def _prepare_alternative_urls(self, original_url: str) -> List[str]:
        """Przygotowuje listƒô alternatywnych URLi do wypr√≥bowania"""
        urls = [original_url]

        # Specjalne przypadki dla problematycznego serwera orka2.sejm.gov.pl
        if 'orka2.sejm.gov.pl' in original_url:
            # Spr√≥buj HTTPS zamiast HTTP
            if original_url.startswith('http://'):
                https_url = original_url.replace('http://', 'https://')
                urls.append(https_url)
                self.logger.debug(f"Dodano alternatywny HTTPS URL: {https_url}")

            # Spr√≥buj inne serwery Sejmu (je≈õli istniejƒÖ)
            alternative_servers = [
                'orka.sejm.gov.pl',
                'www.sejm.gov.pl'
            ]

            parsed_url = urlparse(original_url)
            for alt_server in alternative_servers:
                alt_url = original_url.replace(parsed_url.netloc, alt_server)
                if alt_url != original_url and alt_url not in urls:
                    urls.append(alt_url)
                    self.logger.debug(f"Dodano alternatywny serwer: {alt_url}")

        return urls

    def _get_optimal_timeout(self, url: str) -> int:
        """Zwraca optymalny timeout dla danego serwera"""
        if 'orka2.sejm.gov.pl' in url:
            # Problematyczny serwer - zwiƒôkszony timeout
            return 60
        elif '.gov.pl' in url:
            # Inne serwery rzƒÖdowe - ≈õredni timeout
            return 45
        else:
            # Standardowy timeout
            return self.config.timeout

    def _try_download_with_retry(self, url: str, **kwargs) -> Optional[requests.Response]:
        """Pr√≥buje pobraƒá URL z mechanizmem retry"""

        for attempt in range(self.config.max_retries):
            try:
                # Op√≥≈∫nienie miƒôdzy zapytaniami
                if attempt > 0:
                    wait_time = min(2 ** attempt, 10)  # Exponential backoff, max 10s
                    self.logger.info(f"‚è±Ô∏è  Czekam {wait_time}s przed pr√≥bƒÖ {attempt + 1}...")
                    time.sleep(wait_time)
                else:
                    time.sleep(self.config.delay_between_requests)

                self.logger.debug(f"Pr√≥ba {attempt + 1}/{self.config.max_retries}: {url}")

                response = self.session.get(url, **kwargs)

                if response.status_code == 200:
                    self.logger.debug(f"‚úÖ Sukces pobierania: {url}")
                    return response

                elif response.status_code == 403:
                    self.logger.warning(f"üö´ Dostƒôp zabroniony (403) dla URL: {url}")
                    break  # Nie pr√≥buj ponownie dla 403

                elif response.status_code == 404:
                    self.logger.warning(f"‚ùå Nie znaleziono (404) dla URL: {url}")
                    break  # Nie pr√≥buj ponownie dla 404

                elif response.status_code in [500, 502, 503, 504]:
                    self.logger.warning(
                        f"üîß B≈ÇƒÖd serwera ({response.status_code}) dla URL: {url} - pr√≥ba {attempt + 1}"
                    )
                    # Kontynuuj retry dla b≈Çƒôd√≥w serwera

                else:
                    self.logger.warning(f"‚ö†Ô∏è  HTTP {response.status_code} dla URL: {url}")
                    if attempt < self.config.max_retries - 1:
                        continue
                    else:
                        break

            except Timeout as e:
                self.logger.warning(
                    f"‚è±Ô∏è  Timeout (pr√≥ba {attempt + 1}/{self.config.max_retries}) dla {url}: {e}"
                )
                if attempt == self.config.max_retries - 1:
                    self.logger.error(f"‚ùå Przekroczono maksymalnƒÖ liczbƒô pr√≥b dla {url}")

            except ConnectionError as e:
                self.logger.warning(
                    f"üåê B≈ÇƒÖd po≈ÇƒÖczenia (pr√≥ba {attempt + 1}/{self.config.max_retries}) dla {url}: {e}"
                )
                if attempt == self.config.max_retries - 1:
                    self.logger.error(f"‚ùå Problemy z po≈ÇƒÖczeniem dla {url}")

            except RequestException as e:
                self.logger.warning(
                    f"üîå B≈ÇƒÖd zapytania (pr√≥ba {attempt + 1}/{self.config.max_retries}): {e}"
                )
                if attempt == self.config.max_retries - 1:
                    self.logger.error(f"‚ùå Wszystkie pr√≥by nie powiod≈Çy siƒô")

        self.logger.error(f"‚ùå Wszystkie pr√≥by pobierania nie powiod≈Çy siƒô dla: {url}")
        return None

    def _parse_pdf_bytes(self, pdf_bytes: bytes, source_url: str) -> str:
        """Ekstraktuje tekst z PDF z bajt√≥w u≈ºywajƒÖc pypdf"""
        try:
            import pypdf
            import io

            if not pdf_bytes or len(pdf_bytes) < 100:
                self.logger.error("‚ùå PDF jest pusty lub uszkodzony")
                return ""

            if not pdf_bytes.startswith(b'%PDF-'):
                self.logger.error("‚ùå Plik nie jest prawid≈Çowym PDF")
                return ""

            pdf_stream = io.BytesIO(pdf_bytes)
            text_parts = []

            # U≈ºywamy pypdf zamiast pdfplumber
            reader = pypdf.PdfReader(pdf_stream)
            total_pages = len(reader.pages)
            self.logger.info(f"üìÑ PDF ma {total_pages} stron")

            for page_num, page in enumerate(reader.pages, 1):  # Logika pƒôtli pozostaje ta sama
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(page_text)

                    if page_num % 50 == 0:
                        self.logger.info(f"üìñ Przetworzono {page_num}/{total_pages} stron")

                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è  B≈ÇƒÖd na stronie {page_num}: {e}")
                    continue

            full_text = '\n\n'.join(text_parts)
            cleaned_text = self._clean_extracted_text(full_text)

            self.logger.info(f"‚úÖ WyciƒÖgniƒôto {len(cleaned_text):,} znak√≥w z PDF (pypdf)")
            return cleaned_text

        except ImportError:
            self.logger.error("‚ùå Brak wsparcia dla PDF - zainstaluj: pip install pypdf")
            return ""
        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd parsowania PDF: {e}")
            return ""

    def _parse_docx_bytes(self, docx_bytes: bytes, source_url: str) -> str:
        """
        Ekstraktuje tekst z DOCX z bajt√≥w ‚Äî WY≈ÅƒÑCZONE
        :deprecated: Ta funkcja jest przestarza≈Ça, obs≈Çuga DOCX zosta≈Ça usuniƒôta.
        """
        self.logger.warning("‚ö†Ô∏è  Obs≈Çuga DOCX zosta≈Ça wy≈ÇƒÖczona w tej wersji. U≈ºyj PDF lub HTML.")
        return ""

    def _parse_html_text(self, html_content: str) -> str:
        """Ekstraktuje tekst z HTML"""
        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(html_content, 'html.parser')

            # Usuwa zbƒôdne elementy
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            # Szuka g≈Ç√≥wnej tre≈õci oraz wzorc√≥w dla stron Sejmu
            content_selectors = [
                # Specjalne dla Sejmu
                'div[id*="stenogram"]',
                'div[class*="stenogram"]',
                'div[class*="transcript"]',
                'div[class*="protokol"]',
                'div[id*="protokol"]',
                '.stenogram-content',
                '.transcript-content',
                # Og√≥lne wzorce
                'main', 'article', '.main-content', '.content', '#content',
                # Fallback
                'body'
            ]

            text_content = ""
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    element = elements[0]
                    text_content = element.get_text(separator=' ', strip=True)
                    if len(text_content) > 200:
                        self.logger.debug(f"U≈ºyto selektora: {selector}")
                        break

            # Je≈õli nadal brak tre≈õci, we≈∫ ca≈Çy tekst
            if not text_content or len(text_content) < 200:
                text_content = soup.get_text(separator=' ', strip=True)

            cleaned_text = self._clean_extracted_text(text_content)
            self.logger.info(f"‚úÖ WyciƒÖgniƒôto {len(cleaned_text):,} znak√≥w z HTML")

            return cleaned_text

        except Exception as e:
            self.logger.error(f"‚ùå B≈ÇƒÖd parsowania HTML: {e}")
            return ""

    def _clean_extracted_text(self, text: str) -> str:
        """Czy≈õci wyciƒÖgniƒôty tekst z PDF/HTML"""
        if not text:
            return ""

        import re

        # Usu≈Ñ nadmiarowe bia≈Çe znaki
        text = re.sub(r'\s+', ' ', text)

        # Usu≈Ñ typowe ≈õmieci z dokument√≥w
        cleanup_patterns = [
            r'JavaScript.*?w≈ÇƒÖcz',
            r'Cookie.*?polityka',
            r'Mapa strony',
            r'Menu g≈Ç√≥wne',
            r'Nawigacja',
            r'Przejd≈∫ do',
            r'Skip to',
            r'Strona \d+ z \d+',  # Numery stron
            r'www\.sejm\.gov\.pl',
            r'¬© Kancelaria Sejmu',
        ]

        for pattern in cleanup_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # Normalizuj polskie znaki
        text = text.replace('\u00a0', ' ')  # non-breaking space
        text = text.replace('\u2013', '-')  # en dash
        text = text.replace('\u2014', '--')  # em dash
        text = text.replace('\u2019', "'")  # right single quotation mark
        text = text.replace('\u201c', '"')  # left double quotation mark
        text = text.replace('\u201d', '"')  # right double quotation mark

        # Ko≈Ñcowe czyszczenie
        text = text.strip()
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # Usu≈Ñ nadmierne puste linie

        return text

    def validate_extracted_text(self, text: str, source_url: str) -> bool:
        """Waliduje czy wyciƒÖgniƒôty tekst jest sensowny stenogram Sejmu"""
        if not text or len(text.strip()) < 500:
            self.logger.warning(f"‚ö†Ô∏è  Tekst za kr√≥tki ({len(text)} znak√≥w) dla {source_url}")
            return False

        # Sprawd≈∫ czy tekst zawiera polskie s≈Çowa charakterystyczne dla stenogram√≥w
        polish_keywords = [
            'posiedzenie', 'marsza≈Çek', 'pose≈Ç', 'pos≈Çanka', 'sejm', 'g≈Çosowanie',
            'komisja', 'sprawozdanie', 'ustawa', 'interpelacja', 'punkt', 'porzƒÖdku',
            'obrady', 'wicemarsza≈Çek', 'przewodniczƒÖcy', 'sekretarz', 'protok√≥≈Ç',
            'rada ministr√≥w', 'rzƒÖd', 'minister', 'klub', 'ko≈Ço', 'poselski'
        ]

        text_lower = text.lower()
        found_keywords = sum(1 for keyword in polish_keywords if keyword in text_lower)

        if found_keywords < 5:
            self.logger.warning(
                f"‚ö†Ô∏è  Tekst mo≈ºe nie byƒá stenogramem - znaleziono tylko {found_keywords} kluczowych s≈Ç√≥w")
            return False

        # Sprawd≈∫ jako≈õƒá tekstu
        printable_ratio = sum(1 for c in text if c.isprintable() or c.isspace()) / len(text) if text else 0

        if printable_ratio < 0.9:
            self.logger.warning(f"‚ö†Ô∏è  Tekst zawiera du≈ºo nieprintable znak√≥w ({printable_ratio:.2%})")
            return False

        # Sprawd≈∫ sensowno≈õƒá s≈Ç√≥w
        words = text.split()
        meaningful_words = sum(1 for word in words if len(word) > 2)
        meaningful_ratio = meaningful_words / len(words) if words else 0

        if meaningful_ratio < 0.7:
            self.logger.warning(f"‚ö†Ô∏è  Tekst ma za ma≈Ço sensownych s≈Ç√≥w ({meaningful_ratio:.2%})")
            return False

        self.logger.info(
            f"‚úÖ Tekst przeszed≈Ç walidacjƒô: {len(text):,} znak√≥w, {found_keywords} kluczowych s≈Ç√≥w")
        return True

    def close(self):
        """Zamyka sesjƒô HTTP"""
        if hasattr(self, 'session'):
            self.session.close()
